elifesciences.org
Keramati and Gutkin. eLife 2014;3:e04811. DOI: 10.7554/eLife.04811 1 of 26
Homeostatic reinforcement learning for 
integrating reward collection and 
physiological stability
Mehdi Keramati1,2*, Boris Gutkin1,3*
1Group for Neural Theory, INSERM U960, Departément des Etudes Cognitives, Ecole 
Normale Supérieure, PSL Research University, Paris, France; 2Gatsby Computational 
Neuroscience Unit, University College London, London, United Kingdom; 3Center for 
Cognition and Decision Making, National Research University Higher School of 
Economics, Moscow, Russia
Abstract Efficient regulation of internal homeostasis and defending it against perturbations 
requires adaptive behavioral strategies. However, the computational principles mediating the 
interaction between homeostatic and associative learning processes remain undefined. Here we 
use a definition of primary rewards, as outcomes fulfilling physiological needs, to build a normative 
theory showing how learning motivated behaviors may be modulated by internal states. Within 
this framework, we mathematically prove that seeking rewards is equivalent to the fundamental 
objective of physiological stability, defining the notion of physiological rationality of behavior. We 
further suggest a formal basis for temporal discounting of rewards by showing that discounting 
motivates animals to follow the shortest path in the space of physiological variables toward the 
desired setpoint. We also explain how animals learn to act predictively to preclude prospective 
homeostatic challenges, and several other behavioral patterns. Finally, we suggest a computational 
role for interaction between hypothalamus and the brain reward system.
DOI: 10.7554/eLife.04811.001
Introduction
Survival requires living organisms to maintain their physiological integrity within the environment. In 
other words, they must preserve homeostasis (e.g. body temperature, glucose level, etc.). Yet, how 
might an animal learn to structure its behavioral strategies to obtain the outcomes necessary to fulfill 
and even preclude homeostatic challenges? Such, efficient behavioral decisions surely should depend 
on two brain circuits working in concert: the hypothalamic homeostatic regulation (HR) system, and 
the cortico-basal ganglia reinforcement learning (RL) mechanism. However, the computational mecha-
nisms underlying this obvious coupling remain poorly understood.
The previously developed classical negative feedback models of HR have tried to explain the hypo-
thalamic function in behavioral sensitivity to the ‘internal’ state, by axiomatizing that animals minimize 
the deviation of some key physiological variables from their hypothetical setpoints (Marieb & Hoehn, 
2012). To this end, a direct corrective response is triggered when a deviation from setpoint is sensed 
or anticipated (Sibly & McFarland, 1974; Sterling, 2012). A key lacuna in these models is how a sim-
ple corrective action (e.g. ‘go eat’) in response to a homeostatic deficit might be translated into a 
complex behavioral strategy for interacting with the dynamic and uncertain external world.
On the other hand, the computational theory of RL has proposed a viable computational account 
for the role of the cortico-basal ganglia system in behavioral adaptation to the ‘external’ environment, 
by exploiting experienced environmental contingencies and reward history ( Sutton & Barto, 1998 ; 
Rangel et al., 2008). Critically, this theory is built upon one major axiom, namely, that the objective of 
*For correspondence: mehdi@
gatsby.ucl.ac.uk (MK); boris.
gutkin@ens.fr (BG)
Competing interests: The 
authors declare that no 
competing interests exist.
Funding: See page 23
Received: 18 September 2014
Accepted: 03 November 2014
Published: 02 December 2014
Reviewing editor: Eve Marder, 
Brandeis University, United 
States
 Copyright Keramati and 
Gutkin. This article is distributed 
under the terms of the Creative 
Commons Attribution License, 
which permits unrestricted use 
and redistribution provided that 
the original author and source 
are credited.
RESEARCH ARTICLE
Neuroscience
Keramati and Gutkin. eLife 2014;3:e04811. DOI: 10.7554/eLife.04811 2 of 26
Research article
behavior is to maximize reward acquisition. Yet, this suite of theoretical models does not resolve how 
the brain constructs the reward itself, and how the variability of the internal state impacts overt behavior.
Accumulating neurobiological evidence indicates intricate intercommunication between the hypo-
thalamus and the reward-learning circuitry (Palmiter, 2007; Yeo & Heisler, 2012; Rangel, 2013). The 
integration of the two systems is also behaviorally manifest in the classical behavioral pattern of antic-
ipatory responding in which, animals learn to act predictively to preclude prospective homeostatic 
challenges. Moreover, the ‘good regulator’ theoretical principle implies that ‘every good regulator of 
a system must be a model of that system’ ( Conant & Ashby, 1970 ), accentuating the necessity of 
learning a model (either explicit or implicit) of the environment in order to regulate internal variables, 
and thus, the necessity of associative learning processes being involved in homeostatic regulation.
Given the apparent coupling of homeostatic and learning processes, here, we propose a formal 
hypothesis for the computations, at an algorithmic level, that may be performed in this biological 
integration of the two systems. More precisely, inspired by previous descriptive hypotheses on the 
interaction between motivation and learning (Hull, 1943; Spence, 1956; Mowrer, 1960), we suggest 
a principled model for how the rewarding value of outcomes is computed as a function of the animal's 
internal state, and of the approximated need-reduction ability of the outcome. The computed reward 
is then made available to RL systems that learn over a state-space including both internal and external 
states, resulting in approximate reinforcement of instrumental associations that reduce or prevent 
homeostatic imbalance.
The paper is structured as follows: After giving a heuristic sketch of the theory, we show several 
analytical, behavioral, and neurobiological results. On the basis of the proposed computational inte -
gration of the two systems, we prove analytically that reward-seeking and physiological stability are 
two sides of the same coin, and also provide a normative explanation for temporal discounting of 
eLife digest Our survival depends on our ability to maintain internal states, such as body 
temperature and blood sugar levels, within narrowly defined ranges, despite being subject to 
constantly changing external forces. This process, which is known as homeostasis, requires humans 
and other animals to carry out specific behaviors—such as seeking out warmth or food—to 
compensate for changes in their environment. Animals must also learn to prevent the potential 
impact of changes that can be anticipated.
A network that includes different regions of the brain allows animals to perform the behaviors 
that are needed to maintain homeostasis. However, this network is distinct from the network that 
supports the learning of new behaviors in general. These two systems must, therefore, interact so 
that animals can learn novel strategies to support their physiological stability, but it is not clear how 
animals do this.
Keramati and Gutkin have now devised a mathematical model that explains the nature of this 
interaction, and that can account for many behaviors seen among animals, even those that might 
otherwise appear irrational. There are two assumptions at the heart of the model. First, it is assumed 
that animals are capable of guessing the impact of the outcome of their behaviors on their internal 
state. Second, it is assumed that animals find a behavior rewarding if they believe that the predicted 
impact of its outcome will reduce the difference between a particular internal state and its ideal 
value. For example, a form of behavior for a human might be going to the kitchen, and an outcome 
might be eating chocolate.
Based on these two assumptions, the model shows that animals stabilize their internal state 
around its ideal value by simply learning to perform behaviors that lead to rewarding outcomes 
(such as going into the kitchen and eating chocolate). Their theory also explains the physiological 
importance of a type of behavior known as ‘delay discounting’. Animals displaying this form of 
behavior regard a positive outcome as less rewarding the longer they have to wait for it. The model 
proves mathematically that delay discounting is a logical way to optimize homeostasis.
In addition to making a number of predictions that could be tested in experiments, Keramati and 
Gutkin argue that their model can account for the failure of homeostasis to limit food consumption 
whenever foods loaded with salt, sugar or fat are freely available.
DOI: 10.7554/eLife.04811.002
Neuroscience
Keramati and Gutkin. eLife 2014;3:e04811. DOI: 10.7554/eLife.04811 3 of 26
Research article
reward. Behaviorally, the theory gives a plausible unified account for anticipatory responding and the 
rise-fall pattern of the response rate. We show that the interaction between the two systems is critical 
in these behavioral phenomena and thus, neither classical RL nor classical HR theories can account for 
them. Neurobiologically, we show that our model can shed light on recent findings on the interaction 
between the hypothalamus and the reward-learning circuitry, namely, the modulation of dopaminergic 
activity by hypothalamic signals. Furthermore, we show how orosensory information can be integrated 
with internal signals in a principled way, resulting in accounting for experimental results on consumma-
tory behaviors, as well as the pathological condition of over-eating induced by hyperpalatability. 
Finally, we discuss limitations of the theory, compare it with other theoretical accounts of motivation 
and internal state regulation, and outline testable predictions and future directions.
Results
Theory sketch
A self-organizing system (i.e. an organism) can be defined as a system that opposes the second law of 
thermodynamics ( Friston, 2010 ). In other words, biological systems actively resist the natural ten -
dency to disorder by regulating their physiological state to fall within narrow bounds. This general 
process, known as homeostasis (Cannon, 1929; Bernard, 1957), includes adaptive behavioral strate-
gies for counteracting and preventing self-entropy in the face of constantly changing environments. In 
this sense, one would expect organisms to reinforce responses that mitigate deviation of the internal 
state from desired ‘setpoints’. This is reminiscent of the drive-reduction theory ( Hull, 1943; Spence, 
1956; Mowrer, 1960) according to which, one of the major mechanisms underlying reward is the use-
fulness of the corresponding outcome in fulfilling the homeostatic needs of the organism ( Cabanac, 
1971). Inspired by these considerations (i.e. preservation of self-order and reduction of deviations), we 
propose a formal definition of primary reward (equivalently: reinforcer, economic utility) as the approx-
imated ability of an outcome to restore the internal equilibrium of the physiological state. We then 
demonstrate that our formal homeostatic reinforcement learning framework accounts for some phe -
nomena that classical drive-reduction was unable to explain.
We first define ‘homeostatic space’ as a multidimensional metric space in which each dimension 
represents one physiologically-regulated variable (the horizontal plane in Figure 1 ). The physio -
logical state of the animal at each time t can be represented as a point in this space, denoted by 
tH hh h1,t 2,t N,t= ( , ,.., ) , where ith ,  indicates the state of the i-th physiological variable. For example, ith ,  can 
refer to the animal's glucose level, body temperature, plasma osmolality, etc. The homeostatic set -
point, as the ideal internal state, can be denoted by NH hh h* ** *
12= ( , ,.., ) . As a mapping from the physio-
logical to the motivational state, we define the ‘drive’ as the distance of the internal state from the 
setpoint (the three-dimensional surface in Figure 1):
()
∗ nNm
t i i itDH h h =1 ,=– ∑  (1)
m and n are free parameters that induce important nonlinear effects on the mapping between home-
ostatic deviations and their motivational consequences. Note that for the simple case of m = n = 1, the 
drive function reduces to Euclidian distance. We will later consider more general nonlinear mappings 
in terms of classical utility theory. We will also discuss that the drive function can be viewed as equiva-
lent to the information-theoretic notion of surprise, defined as the negative log-probability of finding 
an organism in a certain state ( ttDH pH() = – l n () ).
Having defined drive, we can now provide a formal definition for primary reward. Let's assume that 
as the result of an action, the animal receives an outcome to  at time t. The impact of this outcome on 
different dimensions of the animal's internal state can be denoted by t t t NtK kk k1, 2, ,= ( , ,.., ) . For example, 
itk ,  can be the quantity of glucose received as a result of outcome to . Hence, the outcome results in a 
transition of the physiological state from tH  to t ttH HK+1 =+  (See Figure 1) and thus, a transition of the 
drive state from tDH()  to t ttDH DH K+1( )= ( + ) . Accordingly, the rewarding value of this outcome can be 
defined as the consequent reduction of drive:
( ) () ( )
() ( )
tt t t
t tt
rH K DH DH
DH DH K
+1,= –
= –+  (2)
Neuroscience
Keramati and Gutkin. eLife 2014;3:e04811. DOI: 10.7554/eLife.04811 4 of 26
Research article
Intuitively, the rewarding value of an outcome depends on the ability of its constituting ele -
ments to reduce the homeostatic distance from the setpoint or equivalently, to counteract self-
entropy. As discussed later, the additive effect ( tK ) of these constituting elements on the internal 
state can be approximated by the orosensory properties of outcomes. We will also discuss how 
erroneous estimation of drive reduction can potentially be a cause for maladaptive consumptive 
behaviors.
We hypothesize in this paper that the primary reward constructed as proposed in Equation 2 is 
used by the brain's reward learning machinery to structure behavior. Incorporating this physiological 
reward definition in a normative RL theory allows us to derive one major result of our theory, which is 
that the rationality of behavioral patterns is geared toward maintaining physiological stability.
Rationality of the theory
Here we show that our definition of reward reconciles the RL and HR theories in terms of their norma-
tive assumptions: reward acquisition and physiological stability are mathematically equivalent behav -
ioral objectives. More precisely, given the proposed definition of reward and given that animals 
discount future rewards (Chung & Herrnstein, 1967), any behavioral policy, π, that maximizes the sum 
of discounted rewards (SDR) also minimizes the sum of discounted deviations from the setpoint, and 
vice versa. In fact, starting from an initial internal state H0, the sum of discounted deviations (SDD) for 
a certain behavioral policy π that causes the internal state to move in the homeostatic space along the 
trajectory p(π), can be defined as:
() ()0 ()
..t
tp
SDD H D H dtπ π
γ=∫  
(3)
Similarly, the sum of discounted rewards (SDR) for a policy π can be defined as:
Figure 1. Schematics of the model in an exemplary two-dimensional homeostatic space. Upon performing an 
action, the animal receives an outcome tK  from the environment. The rewarding value of this outcome depends on 
its ability to make the internal state, tH , closer to the homeostatic setpoint, H*, and thus reduce the drive level (the 
vertical axis). This experienced reward, denoted by ()ttrHK , , is then learned by an RL algorithm. Here a model-free 
RL algorithm is shown in which a reward prediction error signal is computed by comparing the realized reward  
and the expected rewarding value of the performed response. This signal is then used to update the subjective 
value attributed to the corresponding response. Subjective values of alternative choices bias the action selection 
process.
DOI: 10.7554/eLife.04811.003
Neuroscience
Keramati and Gutkin. eLife 2014;3:e04811. DOI: 10.7554/eLife.04811 5 of 26
Research article
() () ( )()∫∫0 +
() ()
= .. = . – .tt
t t t dt
pp
SDR H r dt D H D H dtπ ππ
γγ
 
(4)
It is then rather straightforward to show that for any initial state H0, we will have (See ‘Materials and 
methods’ for the proof):
() ()if SDD H SDR H00  < 1  :       argmin = argmaxππ
ππ
γ
 
(5)
where γ is the discount factor. In other words, the same behavioral policy satisfies optimal reward-
seeking as well as optimal homeostatic maintenance. In this respect, reward acquisition sought by the 
RL system is an efficient means to guide an animal's behavior toward fulfilling the basic objective of 
defending homeostasis. Thus, our theory suggests a physiological basis for the rationality of reward 
seeking.
Normative role of temporal discounting
In the domain of animal behavior, one fundamental question is why animals should discount rewards 
the further they are in the future. Our theory indicates that reward seeking without discounting (i.e., if 
γ = 1) would not lead, and may even be detrimental, to physiological stability (See ‘Materials and 
methods’). Intuitively, this is because a future-discounting agent would always tend to expedite bigger 
rewards and postpone punishments. Such an agent, therefore, tries to reduce homeostatic deviations 
(which is rewarding) as soon as possible, and thus, tries to find the shortest path toward the setpoint. 
A non-discounting agent, in contrast, can always compensate for a deviation-induced punishment by 
reducing that deviation any time in the future.
While the formal proof of the necessity of discounting is given in the ‘Materials and methods’, let 
us give an intuitive explanation. Imagine you had to plan a 1-hr hill walk from a drop-point toward a 
pickup point, during which you wanted to minimize the height (equivalent to drive) summed over the 
path you take. In this summation, if you give higher weights to your height in the near future as com -
pared to later times, the optimum path would be to descend the hill and spend as long as possible 
at the bottom (i.e. homeostatic setpoint) before returning to the pickup point. Equation 5  shows 
that this optimization is equivalent to optimizing the total discounted rewards along the path, given 
that descending and ascending steps are defined as being rewarding and punishing, respectively 
(Equation 2).
In contrast, if at all points in time you give equal weights to your height, then the summed height 
over path only depends on the drop and pickup points, since every ascend can be compensated with 
a descend at any time. In other words, in the absence of discounting, the rewarding value of a behavioral 
policy that changes the internal state only depends on the initial and final internal states, regardless of 
its trajectory in the homeostatic space. Thus, when γ = 1, the values of any two behavioral policies with 
equal net shifts of the internal state are equal, even if one policy moves the internal state along the 
shortest path, whereas the other policy results in large deviations of the internal state from the set -
point and threatens survival. These results hold for any form of temporal discounting (e.g., exponen -
tial, hyperbolic). In this respect, our theory provides a normative explanation for the necessity of 
temporal discounting of reward: to maintain internal stability, it is necessary to discount future rewards.
A normative account of anticipatory responding
A paradigmatic example of behaviors governed by the internal state is the anticipatory responses 
geared to preclude perturbations in regulated variables even before any physiological depletion (neg-
ative feedback) is detectable. Anticipatory eating and drinking that occur before any discernible 
homeostatic deviation (Woods & Seeley, 2002), anticipatory shivering in response to a cue that pre -
dicts the cold (Mansfield et al., 1983; Hjeresen et al., 1986), and insulin secretion prior to meal initi-
ation (Woods, 1991), are only a few examples of anticipatory responding.
One clear example of a conditioned homeostatic response is animals' progressive tolerance to 
ethanol-induced hypothermia. Experiments show that when ethanol injections are preceded (i.e., are 
predictable) by a distinctive cue, the ethanol-induced drop of the body core temperature of animals 
diminishes along the trials (Mansfield & Cunningham, 1980). Figure 2 shows that when the temper-
ature was measured 30, 60, 90, and 120 min after daily injections, the drop of temperature below 
the baseline was significant on the first day, but gradually disappeared over 8 days. Interestingly, in the 
