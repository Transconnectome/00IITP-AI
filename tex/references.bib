@article{behrens2025titans,
  title={Titans: Learning to Memorize at Test Time},
  author={Behrens, Timothy and others},
  journal={arXiv preprint arXiv:2501.00663},
  year={2025}
}

@inproceedings{wang2024brainmamba,
  title={BrainMamba: Efficient dynamic learning on brain networks with selective state space models},
  author={Wang, X. and others},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2024}
}

@article{li2024graphmamba,
  title={Learning dynamic brain network representations with Graph Mamba},
  author={Li, Y. and others},
  journal={IEEE Transactions on Neural Networks and Learning Systems (Early Access)},
  year={2024}
}

@article{gu2022s4,
  title={Efficiently Modeling Long Sequences with Structured State Spaces},
  author={Gu, Albert and Goel, Karan and Re, Christopher},
  journal={ICLR},
  year={2022}
}

@article{hasani2021liquid,
  title={Liquid Time-constant Networks},
  author={Hasani, Ramin and Lechner, Mathias and Amini, Alexander and Rus, Daniela and Grosu, Radu},
  journal={Proceedings of the AAAI Conference on Artificial Intelligence},
  year={2021}
}

@article{bertram2026tubularity,
  title={Tubularity: The Geometry of Robust Representation},
  author={Bertram, Sabina and others},
  journal={Nature Machine Intelligence (Submitted)},
  year={2026}
}

@techreport{rfp2026iitp,
  title={2026 IITP R\&D Planning Request: Embodied AI},
  author={IITP},
  institution={Institute for Information \& Communication Technology Planning \& Evaluation},
  year={2026}
}

@article{friston2010free,
  title={The free-energy principle: a unified brain theory?},
  author={Friston, Karl},
  journal={Nature Reviews Neuroscience},
  year={2010}
}

@article{valencia2025omnifield,
  title={OmniField: Conditioned Neural Fields for Robust Multimodal Spatiotemporal Learning},
  author={Valencia, Kevin and Balasooriya, Thilina and Luo, Xihaier and Park, David Keetae},
  journal={arXiv preprint arXiv:2511.02205},
  year={2025}
}

@article{lee2024strategic,
  title={Strategic Reasoning in Large Language Models: A Graph-Based Approach},
  author={Lee, Moontae and others},
  journal={LG AI Research Technical Report},
  year={2024}
}

@inproceedings{han2024simulated,
  title={Simulated Annealing in Early Layers Leads to Better Generalization},
  author={Han, Danny Dongyeop and others},
  booktitle={Conference on Neural Information Processing Systems (NeurIPS)},
  year={2024},
  url={https://openreview.net/forum?id=6hoQ6PlLCR}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and others},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{he2022masked,
  title={Masked autoencoders are scalable vision learners},
  author={He, Kaiming and others},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  year={2022}
}

@article{dehaene2017consciousness,
  title={What is consciousness, and could machines have it?},
  author={Dehaene, Stanislas and Lau, Hakwan and Kouider, Sid},
  journal={Science},
  volume={358},
  number={6362},
  year={2017}
}

@article{benara2025braintuning,
  title={Brain-tuning: aligning language models with brain recordings improves semantic understanding},
  author={Benara, Omer and Toneva, Mariya},
  journal={arXiv preprint arXiv:2506.03832},
  year={2025}
}

@article{policzer2025multimodal,
  title={The One Where They Brain-Tune for Social Cognition: Multi-Modal Brain-Tuning on Friends},
  author={Policzer, Nico and Braunstein, Cameron and Toneva, Mariya},
  journal={arXiv preprint arXiv:2511.07988},
  year={2025}
}
